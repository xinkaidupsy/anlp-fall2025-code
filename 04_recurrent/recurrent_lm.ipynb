{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Recurrent Neural Networks\n",
    "\n",
    "Lecture 4 | CMU ANLP Fall 2025 | Instructor: Sean Welleck\n",
    "\n",
    "### Part 1: Recurrent language model\n",
    "\n",
    "This is a notebook for [CMU CS11-711 Advanced NLP](https://cmu-l3.github.io/anlp-fall2025/) that trains a recurrent language model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:31.945512Z",
     "start_time": "2025-11-12T03:53:31.932518Z"
    }
   },
   "source": [
    "data = open('names.txt').read().splitlines()\n",
    "data[:10]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:31.977028Z",
     "start_time": "2025-11-12T03:53:31.973896Z"
    }
   },
   "source": [
    "token_to_index = {tok: i for i, tok in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "# Start/stop token\n",
    "token_to_index['[S]'] = 26\n",
    "# Padding token\n",
    "token_to_index['[PAD]'] = 27\n",
    "\n",
    "index_to_token = {i: tok for tok, i in token_to_index.items()}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:32.001157Z",
     "start_time": "2025-11-12T03:53:31.996444Z"
    }
   },
   "source": [
    "token_to_index['[S]']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:45:09.608005Z",
     "start_time": "2025-11-12T04:45:08.469662Z"
    }
   },
   "source": [
    "def build_dataset(data):\n",
    "    X, Y = [], []\n",
    "    for item in data:\n",
    "        tokens = ['[S]'] + list(item) + ['[S]']\n",
    "        indices = [token_to_index[token] for token in tokens]\n",
    "        X.append(indices[:-1])\n",
    "        Y.append(indices[1:])\n",
    "    return X, Y\n",
    "\n",
    "# Split into train, dev, test\n",
    "import random\n",
    "random.seed(123)\n",
    "random.shuffle(data)\n",
    "\n",
    "n1 = int(0.8 * len(data))\n",
    "n2 = int(0.9 * len(data))\n",
    "\n",
    "X_train, Y_train = build_dataset(data[:n1])\n",
    "X_dev, Y_dev = build_dataset(data[n1:n2])\n",
    "X_test, Y_test = build_dataset(data[n2:])\n",
    "\n",
    "len(X_train), len(Y_train)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 25626)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:34.261136Z",
     "start_time": "2025-11-12T03:53:34.257395Z"
    }
   },
   "source": [
    "print(X_train[1], data[1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 18, 7, 0, 8, 13] shain\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:34.307796Z",
     "start_time": "2025-11-12T03:53:34.300288Z"
    }
   },
   "source": [
    "X_train[0], len(X_train[0]), X_train[1], len(X_train[1]), max(len(x) for x in X_train)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([26, 11, 20, 0, 13, 13], 6, [26, 18, 7, 0, 8, 13], 6, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:34.335625Z",
     "start_time": "2025-11-12T03:53:34.331693Z"
    }
   },
   "source": [
    "# Write our own RNN cell \n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Wx = nn.Linear(input_size, hidden_size)\n",
    "        self.Wh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        h = self.activation(self.Wh(h) + self.Wx(x))\n",
    "        return h"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:34.975469Z",
     "start_time": "2025-11-12T03:53:34.353756Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.Wx = nn.Linear(input_size, hidden_size)\n",
    "        self.Wh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        return self.activation(self.Wx(x) + self.Wh(h))\n",
    "\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)   # (V,H)\n",
    "        # expects (B,H)\n",
    "        self.rnn = RNNCell(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)          # (H,V)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(x.size(0), self.hidden_size)     # (B,H)\n",
    "\n",
    "        x = self.embedding(x)                                     # (B,T,H)\n",
    "        outs = []\n",
    "        for i in range(x.size(1)):\n",
    "            xi = x[:, i, :]                                       # (B,H)\n",
    "            hidden = self.rnn(xi, hidden)                         # (B,H)\n",
    "            out = self.output(hidden)                             # (B,V)\n",
    "            outs.append(out.unsqueeze(1))\n",
    "        outs = torch.cat(outs, dim=1)                             # (B,T,V)\n",
    "        return outs, hidden\n",
    "\n",
    "model = RNNLM(len(token_to_index), 32)\n",
    "\n",
    "x = torch.tensor(X_train[:1])\n",
    "\n",
    "output, hidden = model(x, hidden=None)\n",
    "output.size(), hidden.size()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 28]), torch.Size([1, 32]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:34.993110Z",
     "start_time": "2025-11-12T03:53:34.985130Z"
    }
   },
   "source": [
    "model = RNNLM(len(token_to_index), 32)\n",
    "\n",
    "x = torch.tensor(X_train[:1])\n",
    "\n",
    "output, hidden = model(x, hidden=None)\n",
    "output.size(), hidden.size()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 28]), torch.Size([1, 32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:35.005956Z",
     "start_time": "2025-11-12T03:53:35.003178Z"
    }
   },
   "source": [
    "x = torch.tensor(X_train[:2])"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:35.038171Z",
     "start_time": "2025-11-12T03:53:35.029671Z"
    }
   },
   "source": [
    "def pad_batch(X_batch, Y_batch, pad_index):\n",
    "    max_len = max(len(x) for x in X_batch)\n",
    "    X_padded = torch.zeros(len(X_batch), max_len, dtype=torch.long) + pad_index\n",
    "    Y_padded = torch.zeros(len(Y_batch), max_len, dtype=torch.long) + pad_index\n",
    "    for i, (x, y) in enumerate(zip(X_batch, Y_batch)):\n",
    "        X_padded[i, :len(x)] = torch.tensor(x)\n",
    "        Y_padded[i, :len(y)] = torch.tensor(y)\n",
    "    return X_padded, Y_padded\n",
    "\n",
    "xp, yp = pad_batch(X_train[:4], Y_train[:4], token_to_index['[PAD]'])\n",
    "\n",
    "print(xp)\n",
    "for x in xp:\n",
    "    print([index_to_token[i.item()] for i in x])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26, 11, 20,  0, 13, 13, 27, 27, 27, 27],\n",
      "        [26, 18,  7,  0,  8, 13, 27, 27, 27, 27],\n",
      "        [26, 17, 20, 15,  4, 17, 19, 27, 27, 27],\n",
      "        [26, 12, 14, 10, 18,  7,  0,  6, 13,  0]])\n",
      "['[S]', 'l', 'u', 'a', 'n', 'n', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[S]', 's', 'h', 'a', 'i', 'n', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[S]', 'r', 'u', 'p', 'e', 'r', 't', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[S]', 'm', 'o', 'k', 's', 'h', 'a', 'g', 'n', 'a']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T03:53:35.060242Z",
     "start_time": "2025-11-12T03:53:35.053795Z"
    }
   },
   "source": [
    "X_batch, Y_batch = pad_batch(X_train[:2], Y_train[:2], token_to_index['[PAD]'])\n",
    "\n",
    "output, hidden = model(X_batch, hidden=None)\n",
    "output.size(), hidden.size()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6, 28]), torch.Size([2, 32]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:27:06.529733Z",
     "start_time": "2025-11-12T04:27:05.878217Z"
    }
   },
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = RNNLM(vocab_size=len(token_to_index), hidden_size=96)\n",
    "# Count model parameters\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# Dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
    "test_data =\n",
    "\n",
    "# Loss function and optimizer\n",
    "# NOTE: We ignore the loss whenever the target token is a padding token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_index['[PAD]'])\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Reshuffle the data\n",
    "    perm = torch.randperm(len(X_train))\n",
    "    X_train = [X_train[i] for i in perm]\n",
    "    Y_train = [Y_train[i] for i in perm]\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train[i:i+batch_size]\n",
    "        Y_batch = Y_train[i:i+batch_size]\n",
    "        X_batch, Y_batch = pad_batch(X_batch, Y_batch, token_to_index['[PAD]'])\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = model(X_batch) # [batch_size, seq_len, vocab_size] _ is used when you want to ignore a value; model outputs output and hidden state\n",
    "        outputs = outputs.view(-1, len(token_to_index)) # [batch_size * seq_len, vocab_size]\n",
    "        Y_batch = Y_batch.view(-1) # [batch_size * seq_len]\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(X_train) // batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Evaluate validation loss\n",
    "    eval_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_dev), batch_size):\n",
    "            X_batch = X_dev[i:i+batch_size]\n",
    "            Y_batch = Y_dev[i:i+batch_size]\n",
    "            X_batch, Y_batch = pad_batch(X_batch, Y_batch, token_to_index['[PAD]'])\n",
    "\n",
    "            outputs, _ = model(X_batch)\n",
    "            outputs = outputs.view(-1, len(token_to_index))\n",
    "            Y_batch = Y_batch.view(-1)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "    avg_eval_loss = eval_loss / (len(X_dev) // batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_eval_loss:.4f}')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not connect to 127.0.0.1: 5678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 24028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_comm.py\", line 443, in start_client\n",
      "    s.connect((host, port))\n",
      "ConnectionRefusedError: [Errno 61] Connection refused\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mConnectionRefusedError\u001B[39m                    Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     29\u001B[39m Y_batch = Y_train[i:i+batch_size]\n\u001B[32m     30\u001B[39m X_batch, Y_batch = pad_batch(X_batch, Y_batch, token_to_index[\u001B[33m'\u001B[39m\u001B[33m[PAD]\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;43mbreakpoint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[32m     33\u001B[39m outputs, _ = model(X_batch) \u001B[38;5;66;03m# [batch_size, seq_len, vocab_size] _ is used when you want to ignore a value; model outputs output and hidden state\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_breakpointhook.py:27\u001B[39m, in \u001B[36mbreakpointhook\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     20\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     21\u001B[39m             pydevd.settrace(\n\u001B[32m     22\u001B[39m                 suspend=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     23\u001B[39m                 trace_only_current_thread=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     24\u001B[39m                 patch_multiprocessing=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     25\u001B[39m                 stop_at_frame=sys._getframe().f_back.f_back,\n\u001B[32m     26\u001B[39m             )\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[43mpydevd_breakpointhook\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_breakpointhook.py:21\u001B[39m, in \u001B[36mbreakpointhook.<locals>.pydevd_breakpointhook\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     19\u001B[39m     suspend_at_builtin_breakpoint()\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m     \u001B[43mpydevd\u001B[49m\u001B[43m.\u001B[49m\u001B[43msettrace\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuspend\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrace_only_current_thread\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpatch_multiprocessing\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop_at_frame\u001B[49m\u001B[43m=\u001B[49m\u001B[43msys\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_getframe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mf_back\u001B[49m\u001B[43m.\u001B[49m\u001B[43mf_back\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1849\u001B[39m, in \u001B[36msettrace\u001B[39m\u001B[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, overwrite_prev_trace, patch_multiprocessing, stop_at_frame)\u001B[39m\n\u001B[32m   1822\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Sets the tracing function with the pydev debug function and initializes needed facilities.\u001B[39;00m\n\u001B[32m   1823\u001B[39m \n\u001B[32m   1824\u001B[39m \u001B[33;03m@param host: the user may specify another host, if the debug server is not in the same machine (default is the local\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1846\u001B[39m \u001B[33;03m    called this method.\u001B[39;00m\n\u001B[32m   1847\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1848\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m _set_trace_lock:\n\u001B[32m-> \u001B[39m\u001B[32m1849\u001B[39m     \u001B[43m_locked_settrace\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1850\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1851\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstdout_to_server\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1852\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstderr_to_server\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1853\u001B[39m \u001B[43m        \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1854\u001B[39m \u001B[43m        \u001B[49m\u001B[43msuspend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1855\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrace_only_current_thread\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1856\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpatch_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1857\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop_at_frame\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1858\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1906\u001B[39m, in \u001B[36m_locked_settrace\u001B[39m\u001B[34m(host, stdout_to_server, stderr_to_server, port, suspend, trace_only_current_thread, patch_multiprocessing, stop_at_frame)\u001B[39m\n\u001B[32m   1904\u001B[39m py_db = PyDB()\n\u001B[32m   1905\u001B[39m pydev_log.debug(\u001B[33m\"\u001B[39m\u001B[33mpydev debugger: process \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m is connecting\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m % os.getpid())\n\u001B[32m-> \u001B[39m\u001B[32m1906\u001B[39m \u001B[43mpy_db\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Note: connect can raise error.\u001B[39;00m\n\u001B[32m   1908\u001B[39m \u001B[38;5;66;03m# Mark connected only if it actually succeeded.\u001B[39;00m\n\u001B[32m   1909\u001B[39m connected = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:688\u001B[39m, in \u001B[36mPyDB.connect\u001B[39m\u001B[34m(self, host, port)\u001B[39m\n\u001B[32m    686\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m host:\n\u001B[32m    687\u001B[39m     \u001B[38;5;28mself\u001B[39m.communication_role = CommunicationRole.CLIENT\n\u001B[32m--> \u001B[39m\u001B[32m688\u001B[39m     s = \u001B[43mstart_client\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    689\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    690\u001B[39m     \u001B[38;5;28mself\u001B[39m.communication_role = CommunicationRole.SERVER\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/_pydevd_bundle/pydevd_comm.py:443\u001B[39m, in \u001B[36mstart_client\u001B[39m\u001B[34m(host, port)\u001B[39m\n\u001B[32m    441\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    442\u001B[39m     s.settimeout(\u001B[32m10\u001B[39m)  \u001B[38;5;66;03m# 10 seconds timeout\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m443\u001B[39m     \u001B[43ms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    444\u001B[39m     s.settimeout(\u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# no timeout after connected\u001B[39;00m\n\u001B[32m    445\u001B[39m     pydev_log.info(\u001B[33m\"\u001B[39m\u001B[33mConnected to: \u001B[39m\u001B[38;5;132;01m{socket}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m.format(socket=s))\n",
      "\u001B[31mConnectionRefusedError\u001B[39m: [Errno 61] Connection refused"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T02:10:56.126291Z",
     "start_time": "2025-11-12T02:10:56.116582Z"
    }
   },
   "source": [
    "# Sample from the model\n",
    "def sample(model, context, max_length=100):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor([[token_to_index['[S]']] + context])\n",
    "        hidden = None\n",
    "        for _ in range(max_length):\n",
    "            y, hidden = model(x, hidden)\n",
    "            y = y[0, -1].softmax(dim=0)\n",
    "            y = torch.multinomial(y, 1)\n",
    "            token = index_to_token[y.item()]\n",
    "            if token == '[S]':\n",
    "                break\n",
    "            output.append(token)\n",
    "            x = y.view(1, 1)\n",
    "    return ''.join(output)"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T02:10:56.335280Z",
     "start_time": "2025-11-12T02:10:56.259494Z"
    }
   },
   "source": [
    "for i in range(10):\n",
    "    print(sample(model, []))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conzleegh\n",
      "vilon\n",
      "grida\n",
      "roven\n",
      "markie\n",
      "rhon\n",
      "jiaduli\n",
      "khyla\n",
      "dreyck\n",
      "charola\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T02:10:56.992633Z",
     "start_time": "2025-11-12T02:10:56.944528Z"
    }
   },
   "source": [
    "prompt = 's'\n",
    "for i in range(10):\n",
    "    out = sample(model, [token_to_index[tok] for tok in prompt])\n",
    "    print(prompt + out)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shaanie\n",
      "sinain\n",
      "srabek\n",
      "saelon\n",
      "soluwail\n",
      "scambry\n",
      "sarami\n",
      "solepdeli\n",
      "sayko\n",
      "sayvion\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested Exercises\n",
    "\n",
    "1. Use `nn.RNN` instead of our `RNNCell`. Do you have to change anything in the implementation?\n",
    "2. Change `nn.RNN` to `nn.GRU`. Do you have to change anything else in the implementation? Does the loss improve?\n",
    "3. Change `nn.RNN` to `nn.LSTM`. Do you have to change anything else in the implementation? Does the loss improve?\n",
    "4. Vary the hyperparameters (e.g., hidden size, batch size, learning rate, number of epochs). Can you find any consistent relationships between hyperparameter(s) and the loss?\n",
    "5. When the validation loss begins to increase, and the training loss is decreasing, we have evidence of **overfitting**. Can you induce this overfitting by changing the hyperparameters?\n",
    "6. Train a recurrent model on a more complex dataset. Use a tokenizer learned with BPE (either one that you train your own, or a pre-existing one)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T06:42:30.793661Z",
     "start_time": "2025-11-12T06:42:30.783082Z"
    }
   },
   "cell_type": "code",
   "source": "data = open('names.txt').read().split()",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T06:42:34.478614Z",
     "start_time": "2025-11-12T06:42:34.465109Z"
    }
   },
   "cell_type": "code",
   "source": "data[:10]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
